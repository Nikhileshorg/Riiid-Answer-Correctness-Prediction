{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data and Importing Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "env = riiideducation.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/train.csv',\n",
    "                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n",
    "                   dtype={'timestamp': 'int64',\n",
    "                          'user_id': 'int32',\n",
    "                          'content_id': 'int16',\n",
    "                          'content_type_id': 'int8',\n",
    "                          'task_container_id': 'int16',\n",
    "                          'answered_correctly':'int8',\n",
    "                          'prior_question_elapsed_time': 'float32',\n",
    "                          'prior_question_had_explanation': 'boolean'}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in question df\n",
    "questions_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/questions.csv',                         \n",
    "                            usecols=[0, 3],\n",
    "                            dtype={'question_id': 'int16',\n",
    "                              'part': 'int8'}\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)\n",
    "elapsed_mean = train.prior_question_elapsed_time.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\n",
    "group1.columns = ['avg_questions']\n",
    "group2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\n",
    "group2.columns = ['avg_questions']\n",
    "group3 = group1 / group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group3['avg_questions_seen'] = group3.avg_questions.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group3.iloc[0].avg_questions_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_final.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_final.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_u2_final.explanation_mean_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\n",
    "results_q_final.columns = ['quest_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\n",
    "results_q2_final.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')\n",
    "\n",
    "question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2.quest_pct = round(question2.quest_pct,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean_user = results_u2_final.explanation_mean_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train.timestamp == 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train.timestamp != 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Validation Set (Most Recent Answers by User) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = train.groupby('user_id').tail(5)\n",
    "train = train[~train.index.isin(validation.index)]\n",
    "len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_val.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_val.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.groupby('user_id').tail(18)\n",
    "train = train[~train.index.isin(X.index)]\n",
    "len(X) + len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_X.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_X.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "X = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "validation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "X.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "validation.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "\n",
    "validation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\n",
    "X[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_mean = question2.quest_pct.mean()\n",
    "question2.quest_pct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "validation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "X.part = X.part - 1\n",
    "validation.part = validation.part - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X['answered_correctly']\n",
    "X = X.drop(['answered_correctly'], axis=1)\n",
    "X.head()\n",
    "\n",
    "y_val = validation['answered_correctly']\n",
    "X_val = validation.drop(['answered_correctly'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]\n",
    "X_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "X['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "X['quest_pct'].fillna(content_mean, inplace=True)\n",
    "X['part'].fillna(4, inplace = True)\n",
    "X['avg_questions_seen'].fillna(1, inplace = True)\n",
    "X['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "X['prior_question_had_explanation_enc'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "X_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "X_val['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "\n",
    "X_val['part'].fillna(4, inplace = True)\n",
    "X['avg_questions_seen'].fillna(1, inplace = True)\n",
    "X_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "X_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting' : 'gbdt',\n",
    "    'max_bin': 800,\n",
    "    'learning_rate': 0.0175,\n",
    "    'num_leaves': 80\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X, y, categorical_feature = ['part', 'prior_question_had_explanation_enc'],free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, categorical_feature = ['part', 'prior_question_had_explanation_enc'], reference=lgb_train, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):    \n",
    "    params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n",
    "            'boosting_type': 'gbdt',\n",
    "            'max_bin': trial.suggest_int('max_bin', 700, 900),\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 16),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 16),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 1.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 1.0),\n",
    "            'early_stopping_rounds': 10\n",
    "            }\n",
    "    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], verbose_eval=1000, num_boost_round=1300)\n",
    "    val_pred = model.predict(X_val)\n",
    "    score = roc_auc_score(y_true, val_pred)\n",
    "    print(f\"AUC = {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value: {}'.format(trial.value))\n",
    "\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(trial.params, lgb_train, valid_sets=[lgb_train, lgb_eval], verbose_eval=1000,num_boost_round=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "y_true = np.array(y_val)\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Feature Importance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the most important features by split\n",
    "lgb.plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions for New Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n",
    "    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n",
    "    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "    test_df['part'] = test_df.part - 1\n",
    "\n",
    "    test_df['part'].fillna(4, inplace = True)\n",
    "    test_df['avg_questions_seen'].fillna(1, inplace = True)\n",
    "    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n",
    "    \n",
    "    test_df['answered_correctly'] =  model.predict(test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']])\n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
